<h1 align="center">Reinforcement Learning </h1>

The repository provides a set of notebooks used during the *Reinforcement Learning* course. These notebooks serve as an introduction to [Gymnasium](https://github.com/Farama-Foundation/Gymnasium), an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, and [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3), a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is suggested to run the notebooks in `Colab`. The notebooks are structured the following way:

- [**Getting starter**](Notebooks/01_getting_started.ipynb) - the notebook contains an example of **Cartpole balancing** problem. It includes examples of how to implement and evaluate a policy, use available APIs, as well as compares two vanilla policies (*Random action* and *Reactive policy*), [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) and [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html).

- [**Gym environment**](Notebooks/02_gym_environment.ipynb) - provides a an example of how to build a customized environment. We create a `Minigolf` environment model where the agent has to hit a ball on a green using a putter in order to reach the hole with the minimum amount of moves. We also evaluate a few vanilla policies (*Do-nothing*, *Max-action* and *Gaussian policies*), [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html), [DDPG](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html) and [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) in the environment.

- [**G(PO)MDP**](Notebooks/03_gpomdp.ipynb) - en example of how to build a customized policy. In this notebook, we implement [Gradient of partially observable Markov decision process (`G(PO)MDP`) algorithm](https://arxiv.org/abs/1106.0665) with Gaussian policy. The validation of the algorithm was performed in **MountainCarContinuous** environment.